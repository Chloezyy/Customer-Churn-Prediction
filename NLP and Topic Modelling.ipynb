{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d828eafb",
   "metadata": {},
   "source": [
    "we use unsupervised learning models to cluster unlabeled documents into different groups, visualize the results and identify their latent topics/structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcf7fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "# import gensim\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb532039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into dataframe\n",
    "df = pd.read_csv('data.tsv', sep='\\t', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65800d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ae5475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing value\n",
    "df.dropna(subset=['review_body'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152fdb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99da37cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18980569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the first 1000 data as our training data\n",
    "data = df.loc[:999, 'review_body'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326bee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27630169",
   "metadata": {},
   "source": [
    "## Part 2: Tokenizing and Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092339d7",
   "metadata": {},
   "source": [
    "Load stopwords and stemmer function from NLTK library. Stop words are words like \"a\", \"the\", or \"in\" which don't convey significant meaning. Stemming is the process of breaking a word down into its root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a997dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use nltk's English stopwords.\n",
    "stopwords = nltk.corpus.stopwords.words('english') #stopwords.append(\"n't\")\n",
    "stopwords.append(\"'s\")\n",
    "stopwords.append(\"'m\")\n",
    "stopwords.append(\"br\") #html <br>\n",
    "stopwords.append(\"watch\")\n",
    "\n",
    "print (\"We use \" + str(len(stopwords)) + \" stop-words from nltk library.\")\n",
    "print (stopwords[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b42844",
   "metadata": {},
   "source": [
    "Use our defined functions to analyze (i.e. tokenize, stem) our reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f09477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# tokenization and stemming\n",
    "def tokenization_and_stemming(text):\n",
    "    tokens = []\n",
    "    # exclude stop words and tokenize the document, generate a list of string\n",
    "    for word in nltk.word_tokenize(text):\n",
    "        if word.lower() not in stopwords:\n",
    "            tokens.append(word.lower())\n",
    "\n",
    "    filtered_tokens = []\n",
    "\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if token.isalpha():\n",
    "            filtered_tokens.append(token)\n",
    "\n",
    "    # stemming\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec017dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ed1709",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization_and_stemming(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c757e3",
   "metadata": {},
   "source": [
    "## Part 3: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04241990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# define vectorizer parameters\n",
    "# TfidfVectorizer will help us to create tf-idf matrix\n",
    "# max_df : maximum document frequency for the given word\n",
    "# min_df : minimum document frequency for the given word\n",
    "# max_features: maximum number of words in the dictionary\n",
    "# use_idf: if not true, we only calculate tf\n",
    "# stop_words : built-in stop words\n",
    "# tokenizer: how to tokenize the document\n",
    "# ngram_range: (min_value, max_value), eg. (1, 3) means the result will include 1-gram, 2-gram, 3-gram\n",
    "tfidf_model = TfidfVectorizer(max_df = 0.99, max_features=1000,\n",
    "                                 min_df = 0.01, stop_words='english',\n",
    "                                 use_idf = True, tokenizer = tokenization_and_stemming, ngram_range=(1,1))\n",
    "\n",
    "tfidf_matrix = tfidf_model.fit_transform(data) #fit the vectorizer to synopses\n",
    "\n",
    "print (\"In total, there are \" + str(tfidf_matrix.shape[0]) + \\\n",
    "      \" reviews and \" + str(tfidf_matrix.shape[1]) + \" terms.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77932e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f11b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix.toarray() #todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af716e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6932143b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix.toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef9f339",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(tfidf_matrix.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4fce70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(tfidf_matrix.todense()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba58c1f7",
   "metadata": {},
   "source": [
    "Save the terms identified by TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee797bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words\n",
    "tf_selected_words = tfidf_model.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4de199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out words\n",
    "tf_selected_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e074db",
   "metadata": {},
   "source": [
    "## Part 4: K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7638057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 5\n",
    "\n",
    "# number of clusters\n",
    "km = KMeans(n_clusters = num_clusters)\n",
    "km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae877bc1",
   "metadata": {},
   "source": [
    "4.1. Analyze K-means Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eb44e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DataFrame films from all of the input files.\n",
    "product = { 'review': df[:1000].review_body, 'cluster': clusters}\n",
    "frame = pd.DataFrame(product, columns = ['review', 'cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efe31b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb184c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Number of reviews included in each cluster:\")\n",
    "frame['cluster'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bebc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "km.cluster_centers_\n",
    "\n",
    "# 239数的list -> cluster 0的中心点的tf-idf值\n",
    "#-> assumption: 中心点的值可以代表这个cluster\n",
    "#-> tf-idf值越大，对应的词越能代表这个document\n",
    "#-> 选出了tf-idf最大的6个值对应的词来代表这个cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25536005",
   "metadata": {},
   "outputs": [],
   "source": [
    "km.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faf379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"<Document clustering result by K-means>\")\n",
    "\n",
    "#km.cluster_centers_ denotes the importances of each items in centroid.\n",
    "#We need to sort it in decreasing-order and get the top k items.\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "Cluster_keywords_summary = {}\n",
    "for i in range(num_clusters):\n",
    "    print (\"Cluster \" + str(i) + \" words:\", end='')\n",
    "    Cluster_keywords_summary[i] = []\n",
    "    for ind in order_centroids[i, :6]: #replace 6 with n words per cluster\n",
    "        Cluster_keywords_summary[i].append(tf_selected_words[ind])\n",
    "        print (tf_selected_words[ind] + \",\", end='')\n",
    "    print ()\n",
    "\n",
    "    cluster_reviews = frame[frame.cluster==i].review.tolist()\n",
    "    print (\"Cluster \" + str(i) + \" reviews (\" + str(len(cluster_reviews)) + \" reviews): \")\n",
    "    print (\", \".join(cluster_reviews))\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704e8036",
   "metadata": {},
   "source": [
    "## Part 5: Topic Modeling - Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a59993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LDA for clustering\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8921ada9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# document topic matrix for tfidf_matrix_lda\n",
    "lda_output = lda.fit_transform(tfidf_matrix)\n",
    "print(lda_output.shape)\n",
    "print(lda_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4e6ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics and words matrix\n",
    "topic_word = lda.components_\n",
    "print(topic_word.shape)\n",
    "print(topic_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d65bcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# column names\n",
    "topic_names = [\"Topic\" + str(i) for i in range(lda.n_components)]\n",
    "\n",
    "# index names\n",
    "doc_names = [\"Doc\" + str(i) for i in range(len(data))]\n",
    "\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topic_names, index=doc_names)\n",
    "\n",
    "# get dominant topic for each document\n",
    "topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['topic'] = topic\n",
    "\n",
    "df_document_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c06dabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_document_topic['topic'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef9c866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic word matrix\n",
    "print(lda.components_)\n",
    "# topic-word matrix\n",
    "df_topic_words = pd.DataFrame(lda.components_)\n",
    "\n",
    "# column and index\n",
    "df_topic_words.columns = tfidf_model.get_feature_names_out()\n",
    "df_topic_words.index = topic_names\n",
    "\n",
    "df_topic_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ab36a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print top n keywords for each topic\n",
    "def print_topic_words(tfidf_model, lda_model, n_words):\n",
    "    words = np.array(tfidf_model.get_feature_names_out())\n",
    "    topic_words = []\n",
    "    # for each topic, we have words weight\n",
    "    for topic_words_weights in lda_model.components_:\n",
    "        top_words = topic_words_weights.argsort()[::-1][:n_words]\n",
    "        topic_words.append(words.take(top_words))\n",
    "    return topic_words\n",
    "\n",
    "topic_keywords = print_topic_words(tfidf_model=tfidf_model, lda_model=lda, n_words=15)\n",
    "\n",
    "df_topic_words = pd.DataFrame(topic_keywords)\n",
    "df_topic_words.columns = ['Word '+str(i) for i in range(df_topic_words.shape[1])]\n",
    "df_topic_words.index = ['Topic '+str(i) for i in range(df_topic_words.shape[0])]\n",
    "df_topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89abd125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c7b971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c8be68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1ec061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2922b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
